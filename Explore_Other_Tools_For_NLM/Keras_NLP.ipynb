{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras and Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sebastian Sierra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline\n",
    "* What is Keras?\n",
    "  * Installing Keras\n",
    "* Models in Keras: Sequential vs Graph\n",
    "* Recurrent layers in keras.\n",
    "  * LSTM and bidirectional LSTM\n",
    "  * GRU\n",
    "* Creating new layers in Keras\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Keras?\n",
    "\n",
    "It's a Deep Learning library for Theano and TensorFlow. Keras is also built upon four guiding principles:\n",
    "* Modularity.\n",
    "  *  Neural layers, cost functions, optimizers, initialization schemes, activation functions, regularization schemes are all standalone modules.\n",
    "* Minimalism.\n",
    "* Easy extensibility.\n",
    "* Work with Python.\n",
    "\n",
    "Keras is suited for easy and fast prototyping. It also supports **convolutional neural networks** and **recurrent neural networks** and easy combination between both. Besides it enables multi-input and multi-output training. Keras runs on GPU or CPU.\n",
    "\n",
    "**Further documentation** can be found on [Keras Docs](http://keras.io/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras requirements are:\n",
    "* numpy, scipy\n",
    "* pyyaml\n",
    "* HDF5 and h5py\n",
    "* In case of using CNNs: cuDNN\n",
    "\n",
    "In this case we are going to work with **Theano** as backend, so the latest version of **Theano** should be used\n",
    "```bash\n",
    "sudo pip install git+git://github.com/Theano/Theano.git\n",
    "```\n",
    "Finally pip install the latest version of keras\n",
    "```bash\n",
    "sudo pip install keras\n",
    "```\n",
    "Then we check if we have the latest version(>0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pkg_resources\n",
    "pkg_resources.get_distribution(\"keras\").version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models in Keras: Sequential vs Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models are the main structure in Keras. There are two kinds of models: Sequential model and Graph model. Sequential is a sequence of layers, organized in the exact order they where added. Graph models are determined by the connections nodes and the connections between their nodes.\n",
    "\n",
    "Sequential models can be easily created:\n",
    "```python\n",
    "from keras.models import Sequential\n",
    "model = Sequential()\n",
    "```\n",
    "Then we can add each layer, in this short example we are creating a network with a Embedding layer as input layer, then we add a LSTM, a Dropout layer, a Dense layer that is a standard fully connected layer and finally an Activation layer using a sigmoid function.\n",
    "```python\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM\n",
    "\n",
    "model.add(Embedding(input_dim, output_dim, input_length=maxlen))\n",
    "model.add(LSTM(output_dim))\n",
    "model.add(Dropout(prob))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "```\n",
    "\n",
    "One example of Keras' easy extensibility is that we can define functions :\n",
    "```python\n",
    "def tanh(x):\n",
    "    return theano.tensor.tanh(x)\n",
    "\n",
    "model.add(Dense(64, activation=tanh))\n",
    "model.add(Activation(tanh))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the other side we have Graph models, that can be created so:\n",
    "```python\n",
    "from keras.models import Sequential\n",
    "model = Graph()\n",
    "```\n",
    "In this case we are defining a bidirectional LSTM for a classification problem. Note that in this case we have to define first the input. *maxlen* stands for the input size that our network will have. The details of the construction of a bidirectional LSTM will be further discussed. At the end of the specification of the network we can see that it is really similar to the specification of the previous network.\n",
    "```python\n",
    "model.add_input(name='input', input_shape=(maxlen,), dtype=int)\n",
    "model.add_node(Embedding(input_dim, output_dim, input_length=maxlen),\n",
    "               name='embedding', input='input')\n",
    "model.add_node(LSTM(output_dim), name='forward', input='embedding')\n",
    "model.add_node(LSTM(output_dim, go_backwards=True), name='backward', input='embedding')\n",
    "model.add_node(Dropout(prob), name='dropout', inputs=['forward', 'backward'])\n",
    "model.add_node(Dense(1, activation='sigmoid'), name='sigmoid', input='dropout')\n",
    "model.add_output(name='output', input='sigmoid')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Layers in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recurrent Layers are implemented in Keras. It supports LSTM, GRU and SimpleRNN recurrent layers. Each of one can be called easiy using this:\n",
    "```python\n",
    "from keras.layers.recurrent import LSTM, GRU, SimpleRNN\n",
    "```\n",
    "Its input is a 3D tensor with shape **(nb_samples, timesteps, input_dim)**. The output will be 3D tensor with shape  **(nb_samples, timesteps, output_dim)**.\n",
    "\n",
    "Keras by default resets the memory of the recurrent network. In some cases we would like to enable statefulness, so the input of the following iteration is fed with the previous state of the network. This can be done specifying `stateful=True` in the layer constructor.\n",
    "\n",
    "We are going to see how a RNN can be used in text classification task and compare the performance of three basic structures: LSTM, GRU and Bidirectional LSTM. Although we have to set our data ready to use in Keras. Keras has a module with some standard datasets, in our case we will work with the sentiment analysis task of the IMDB reviews dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: Graphics Device (CNMeM is disabled)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "np.random.seed(1337) # for reproducibility\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.optimizers import SGD, RMSprop, Adagrad\n",
    "from keras.utils import np_utils\n",
    "from keras.utils.np_utils import accuracy\n",
    "from keras.models import Sequential, Graph\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "from keras.datasets import imdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to define the number of top most frequent words to consider of our Embedding layer, this number will be *max_features*, then we define the maximum length of the input sequence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_features = 20000\n",
    "maxlen = 100 # cut texts after this number of words (among top max_features most common words)\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we easily load the IMDB data, defining the percentage for test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000 train sequences\n",
      "5000 test sequences\n"
     ]
    }
   ],
   "source": [
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(nb_words=max_features, test_split=0.2)\n",
    "print(len(X_train), 'train sequences')\n",
    "print(len(X_test), 'test sequences')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the sequences will be padded(where the length is less than 100):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad sequences (samples x time)\n",
      "X_train shape: (20000, 100)\n",
      "X_test shape: (5000, 100)\n"
     ]
    }
   ],
   "source": [
    "print(\"Pad sequences (samples x time)\")\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=maxlen)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=maxlen)\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('X_test shape:', X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we build the model as we have previously done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128, input_length=maxlen))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With compile function we can set the objective function, the optimizer and the class evaluation mode. The following objectives are available:\n",
    "* mean_squared_error / mse\n",
    "* root_mean_squared_error / rmse\n",
    "* mean_absolute_error / mae\n",
    "* mean_absolute_percentage_error / mape\n",
    "* mean_squared_logarithmic_error / msle\n",
    "* squared_hinge\n",
    "* hinge\n",
    "* binary_crossentropy: Also known as logloss.\n",
    "* categorical_crossentropy: multiclass logloss\n",
    "\n",
    "On the side of the optimizers, Keras provide us these:\n",
    "* SGD\n",
    "* RMSprop\n",
    "* Adagrad\n",
    "* Adadelta\n",
    "* Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', class_mode=\"binary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can use *fit* function(In a sci-kit learn fashion) to train the model. *evaluate* will show the performance of the model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/4\n",
      "20000/20000 [==============================] - 41s - loss: 0.4650 - acc: 0.7778 - val_loss: 0.3676 - val_acc: 0.8352\n",
      "Epoch 2/4\n",
      "20000/20000 [==============================] - 41s - loss: 0.2663 - acc: 0.8947 - val_loss: 0.3671 - val_acc: 0.8418\n",
      "Epoch 3/4\n",
      "20000/20000 [==============================] - 41s - loss: 0.1635 - acc: 0.9397 - val_loss: 0.3994 - val_acc: 0.8318\n",
      "Epoch 4/4\n",
      "20000/20000 [==============================] - 41s - loss: 0.0985 - acc: 0.9653 - val_loss: 0.5071 - val_acc: 0.8168\n",
      "5000/5000 [==============================] - 2s     \n",
      "Test score: 0.507146091509\n",
      "Test accuracy: 0.8168\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=4, validation_data=(X_test, y_test), show_accuracy=True)\n",
    "score, acc = model.evaluate(X_test, y_test, batch_size=batch_size, show_accuracy=True)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can easily use a GRU instead of a LSTM. Most of the code will be similar to the previous one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------\n",
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/4\n",
      "20000/20000 [==============================] - 34s - loss: 0.5583 - acc: 0.6795 - val_loss: 0.3636 - val_acc: 0.8364\n",
      "Epoch 2/4\n",
      "20000/20000 [==============================] - 34s - loss: 0.2841 - acc: 0.8841 - val_loss: 0.3521 - val_acc: 0.8490\n",
      "Epoch 3/4\n",
      "20000/20000 [==============================] - 35s - loss: 0.1705 - acc: 0.9388 - val_loss: 0.4167 - val_acc: 0.8402\n",
      "Epoch 4/4\n",
      "20000/20000 [==============================] - 35s - loss: 0.1018 - acc: 0.9683 - val_loss: 0.5037 - val_acc: 0.8306\n",
      "5000/5000 [==============================] - 1s     \n",
      "Test score: 0.503725951719\n",
      "Test accuracy: 0.8306\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128, input_length=maxlen))\n",
    "model.add(GRU(128))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', class_mode=\"binary\")\n",
    "\n",
    "print(\"----------------------\")\n",
    "model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=4, validation_data=(X_test, y_test), show_accuracy=True)\n",
    "score, acc = model.evaluate(X_test, y_test, batch_size=batch_size, show_accuracy=True)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following case we will use a little more complicated structure. A bidirectional LSTM, it will be built using a Graph model. The main key is to declare two LSTM, one of them have to be enabled to go backward. Unfortunately documentation about this functionality is not clear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Graph()\n",
    "model.add_input(name='input', input_shape=(maxlen,), dtype=int)\n",
    "model.add_node(Embedding(max_features, 128, input_length=maxlen),\n",
    "               name='embedding', input='input')\n",
    "model.add_node(LSTM(64), name='forward', input='embedding')\n",
    "model.add_node(LSTM(64, go_backwards=True), name='backward', input='embedding')\n",
    "model.add_node(Dropout(0.5), name='dropout', inputs=['forward', 'backward'])\n",
    "model.add_node(Dense(1, activation='sigmoid'), name='sigmoid', input='dropout')\n",
    "model.add_output(name='output', input='sigmoid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time instead of using *evaluate* function, we will evaluate it manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "Epoch 1/4\n",
      "20000/20000 [==============================] - 68s - loss: 0.4806    \n",
      "Epoch 2/4\n",
      "20000/20000 [==============================] - 69s - loss: 0.2674    \n",
      "Epoch 3/4\n",
      "20000/20000 [==============================] - 68s - loss: 0.1510    \n",
      "Epoch 4/4\n",
      "20000/20000 [==============================] - 68s - loss: 0.0706    \n",
      "Test accuracy: 0.8354\n"
     ]
    }
   ],
   "source": [
    "model.compile('adam', {'output': 'binary_crossentropy'})\n",
    "\n",
    "print('--------------------')\n",
    "model.fit({'input': X_train, 'output': y_train}, batch_size=batch_size, nb_epoch=4)\n",
    "acc = accuracy(y_test, np.round(np.array(model.predict({'input': X_test},\n",
    "                                               batch_size=batch_size)['output'])))\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating new layers in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Keras and gensim to solve Semantic Similarity task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About us\n",
    "<img src=\"https://sites.google.com/a/unal.edu.co/mindlab/_/rsrc/1353286903227/config/customLogo.gif?revision=10\" />"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
