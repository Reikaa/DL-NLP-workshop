{"name":"Deep Learning and Natural Language Processing - workshop","tagline":"","body":"# Deep Learning and Natural Language Processing workshop.\r\nThis workshop will be held at Universidad Nacional de Colombia. Building 453(Aulas de ingeniería). We will show how deep learning meets natural language processing through 4 sessions. Each session will have a presentation and a corresponding jupyter notebook.\r\n\r\n## Schedule.\r\n* Tuesday 12th morning:    \r\n    Link presentation:    \r\n    Link notebook:    \r\n* Tuesday 12th afternoon:    \r\n    Link presentation:    \r\n    Link notebook:\r\n* Wednesday 13th morning:    \r\n    Link presentation:    \r\n    Link notebook:\r\n* Wednesday 13th afternoon: Keras and Paraphrasing by Sebastian Sierra.    \r\n    Link presentation: Yet to come.    \r\n    Link notebook: Yet to come.\r\n\r\n## Useful links and resources.\r\n### Neural networks tools.    \r\nThis is a curated list of neural networks tools and frameworks. Most of them are written in Python and built on top of Theano. Most of them will be available from hal and lisi4 server.    \r\n* [Theano](http://deeplearning.net/software/theano/): A numerical computation library for Python.\r\n* [Blocks](https://github.com/mila-udem/blocks): Framework that helps you build neural network models on top of Theano.\r\n* [Torch](http://torch.ch/): Scientific computing framework with wide support for machine learning algorithms using Lua. \r\n* [Keras](http://keras.io/): A minimalist, highly modular neural networks library, written in Python and capable of running on top of either TensorFlow or Theano.\r\n* [TensorFlow](https://www.tensorflow.org/): An open source software library for numerical computation using data flow graphs.\r\n* Gensim's [word2vec](https://radimrehurek.com/gensim/models/word2vec.html): An useful library built in Python that helps you build and train word2vec models.\r\n\r\n### Surveys.\r\nSurveys and empirical studies are useful resources describing how Recurrent Neural Networks behave. During 2015 there were several works related to this topic and these are some of the most important.    \r\n* Greff, K., Kumar Srivastava, R., Koutník, J., Steunebrink, B. R., & Schmidhuber, J. (2015). [LSTM: A Search Space Odyssey](http://arxiv.org/abs/1503.04069).\r\n* Karpathy, A., Johnson, J., & Fei-Fei, L. (2015). [Visualizing and Understanding Recurrent Networks](http://arxiv.org/abs/1506.02078).\r\n* Zachary C. Lipton. (2015). [A Critical Review of Recurrent Neural Networks for Sequence Learning](http://arxiv.org/abs/1506.00019v2).\r\n* Jozefowicz, R., Zaremba, W., & Sutskever, I. (2015). [An Empirical Exploration of Recurrent Network Architectures](http://machinelearning.wustl.edu/mlpapers/paper_files/icml2015_jozefowicz15.pdf). In Proceedings of the 32nd International Conference on Machine Learning (Vol. 37).\r\n\r\n### Natural Language Processing.\r\n* [CS224d](http://cs224d.stanford.edu/index.html) Stanford Deep Learning for Natural Language Processing: Course materials [available](http://cs224d.stanford.edu/syllabus.html).\r\n\r\n### Recurrent neural networks.\r\nAlthough these resources could be in the NLP section, these are created as blogs or pages showing the power of Recurrent neural networks.    \r\n* [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) by Andrej Karpathy.\r\n* [Understanding LSTMs](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) by Christopher Colah.\r\n* [Recurrent Neural Networks tutorial](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/) by [wild-ml](http://www.wildml.com/) blog.\r\n\r\n### Applications \r\nThere are several applications of RNN for Natural Language Processing. Language Modelling, Text classification and Machine translation are some of the them.\r\n#### Language modelling\r\n* Kiros, R., Zhu, Y., Salakhutdinov, R., Zemel, R. S., Torralba, A., Urtasun, R., & Fidler, S. (2015). [Skip-Thought Vectors](https://github.com/ryankiros/skip-thoughts). arXiv Preprint arXiv:1506.06726.\r\n* Li, J., Luong, M., & Jurafsky, D. (2015). [A Hierarchical Neural Autoencoder for Paragraphs and Documents](http://arxiv.org/pdf/1506.01057). In Acl 2015.\r\n* Tai, K. S., Socher, R., & Manning, C. D. (2015). [Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks](http://arxiv.org/abs/1503.00075). arXiv Preprint arXiv:1503.00075.\r\n\r\n## Further resources.\r\nThis list of links is based on Ji won kim's [Awesome RNN](http://jiwonkim.org/awesome-rnn) list of resources.\r\n\r\n## About us\r\n![MindLab Research Group](http://i.imgur.com/CLrXU3M.png)\r\n\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}